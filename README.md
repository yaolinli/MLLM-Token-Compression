# Towards Efficient Multimodal Large Language Models: A Survey on Token Compression

> **üì¢ Contributions Welcome**
>
> We appreciate contributions that help improve this repository and the accompanying paper. Please feel free to submit a [pull request](https://github.com/DuHeng0128/Awesome-Token-Compression/pulls) to:
> 1. Add a missing or relevant paper.
> 2. Propose a more suitable category or tag.
> 3. Update or correct information (links, metadata, status).
> 4. Addressing Potential Issues in Benchmarking.
> 5. Request clarification or report an issue.
> 
> Thank you ‚Äî every suggestion helps make this resource more useful.

## ‚úíÔ∏è Table of Contents
- [News](#-news): Latest Updates, News, and Announcements.
- [About](#-about): Overview and Objectives.
- [Tag Description](#-tag-description): Brief Explanation of Tags in Paper Table.
- [Paper Table](#-paper-table): Paper Index (by Year, Descending).
- [Benchmark](): An overview of our proposed benchmark for MLLM token compression .
- [Citation](#-citation): If you find this helpful, please consider citing us.

## üî• News
- **[2025.11.26]** We've released the repository! The first version of survey will be published soon. üéâüéâ

## ‚òÄÔ∏è About
Multimodal Large Language Models (MLLMs) are rapidly expanding their capabilities, but high-resolution images and long videos create extremely long visual-token streams that dramatically increase compute, memory, and latency requirements. This repository accompanies our survey on **Towards Efficient Multimodal Large Language Models: A Survey on Token Compression** ([Techriv]()) to help researchers and practitioners navigate this field.

**Motivation.** Token compression reduces the number of visual tokens processed by MLLMs while preserving critical cross-modal semantics, enabling more efficient training and faster inference without large accuracy regressions. The field is fragmented across encoders, projectors, and LLM-side techniques; a centralized, searchable resource is needed.

**Target audience.** Researchers, implementers, and system designers working on multimodal models, retrieval, efficient vision-language pipelines, and deployment at scale.

**What this repo provides.**
- A curated, searchable, chronologically organized **paper index**.
- Short annotated entries with metadata (method family, compression ratio, retrain vs plugin, modality).
- Links to code, checkpoints, and reproducibility notes where available.
- An overview of our proposed **evaluation suite** for MLLM token compression and benchmarks.
- Contribution guidelines and templates for adding papers or methods.

Feel free to browse the table, open issues, or contribute entries to help grow a rigorous, practical ecosystem for efficient multimodal modeling.


## üìã Tag Description
- ![arXiv Badge](https://img.shields.io/badge/arXiv-red) `red` for arXiv papers
- ![PDF Badge](https://img.shields.io/badge/PDF-blue) `blue` for conference/journal papers
- ![GitHub Badge](https://img.shields.io/badge/GitHub-white) `white` for GitHub repositories
- ![Research Areas Badge](https://img.shields.io/badge/Modality-purple) `purple` for modality
- ![Position Badge](https://img.shields.io/badge/Compression_Position-cyan) `cyan` for compression position
- [![Text Query Badge](https://img.shields.io/badge/Text_Query--Based_(TQ)-Yes-yes)]() `brightgreen` for whether it is text query-based
- [![Method Badge](https://img.shields.io/badge/Compression_Method-lightgrey)]() `lightgrey` for compression methods: merge or pruning
- [![Mode Badge](https://img.shields.io/badge/Usage_Mode-yellow)]() `yellow` for usage mode: re-train or plug-in
- [![Speed Badge](https://img.shields.io/badge/Speed_Stage-orange)]() `orange` for acceleration stage: Train stage or Inference Stage
- [![Ratio Badge](https://img.shields.io/badge/Compression_Ratio-pink)]() `pink` for compression ratio: fix or dynamic
- [![Train_Infer Badge](https://img.shields.io/badge/Train_or_Infer-yellowgreen)]() `yellowgreen` for usage stage

## üìö Paper Table

| **Title & Authors** | **Date** | **Links** | **Modality & Position** | **Tags** |
| --- | --- | ---  | --- | :---: | 
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/Chenfei-Liao/VTC-Bench.svg?style=social&label=Star)]()<br>[Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods](https://arxiv.org/pdf/2510.07143) <br> Chenfei Liao, Wensong Wang, Zichen Wen, Xu Zheng, Yiyu Wang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Xin Zou, Yuqian Fu, Bin Ren, Linfeng Zhang, Xuming Hu | 2025/10 | [GitHub](https://github.com/Chenfei-Liao/VTC-Bench)<br> | - | - |
| [![PDF](https://img.shields.io/badge/Neurips-2025-blue)]() [![Star](https://img.shields.io/github/stars/obananas/HoloV.svg?style=social&label=Star)]()<br>[Don‚Äôt Just Chase ‚ÄúHighlighted Tokens‚Äù in MLLMs: Revisiting Visual Holistic Context Retention](https://arxiv.org/pdf/2510.02912) <br> Xin Zou, Di Lu, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Xu Zheng, Linfeng Zhang, Xuming Hu | 2025/10 | [GitHub](https://github.com/obananas/HoloV) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance](https://arxiv.org/pdf/2509.15704) <br> Yuxuan Liang, Xu Li, Xiaolei Chen, Yi Zheng, Haotian Chen, Bin Li, Xiangyang Xue | 2025/09 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/NIneeeeeem/LangDC.svg?style=social&label=Star)]()<br>[Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors](https://arxiv.org/pdf/2509.00969) <br> Xiangchen Wang, Jinrui Zhang, Teng Wang, Haigang Zhang, Feng Zheng | 2025/09 | [GitHub](https://github.com/NIneeeeeem/LangDC)<br>[Model](https://huggingface.co/Wangxc1000/LangDC) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2509.01552) [![Star](https://img.shields.io/github/stars/xuyang-liu16/V2Drop.svg?style=social&label=Star)]()<br>[Variation-aware Vision Token Dropping for Faster Large Vision-Language Models](https://arxiv.org/pdf/2509.01552) <br> Junjie Chen, Xuyang Liu, Zichen Wen, Yiyu Wang, Siteng Huang, Honggang Chen | 2025/09 | [GitHub](https://github.com/xuyang-liu16/V2Drop) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[TrimTokenator: Towards Adaptive Visual Token Pruning for Large Multimodal Models](https://arxiv.org/pdf/2509.00320.pdf) <br> Hao Zhang, Mengsi Lyu, Chenrui He, Yulong Ao, Yonghua Lin | 2025/09 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs](https://arxiv.org/pdf/2508.21044) <br> Junpeng Ma, Qizhe Zhang, Ming Lu, Zhibin Wang, Qiang Zhou, Jun Song, Shanghang Zhang | 2025/08 | - | [![Area](https://img.shields.io/badge/Video-purple)]() | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering](https://arxiv.org/pdf/2508.17860v1) <br> Kang Zeng, Guojin Zhong, Jintao Cheng, Jin Yuan, Zhiyong Li | 2025/08 | - | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/pdf/2508.17243v1) <br> Zicong Tang, Ziyang Ma, Suqing Wang, Zuchao Li, Lefei Zhang, Hai Zhao, Yun Li, Qianren Wang | 2025/08 | - | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[PoRe: Position-Reweighted Visual Token Pruning for Vision Language Models](https://arxiv.org/pdf/2508.17807v1) <br> Kai Zhao, Wubang Yuan, Alex Lingyu Hung, Dan Zeng | 2025/08 | - | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/Ironieser/MMTok.svg?style=social&label=Star)]()<br>[MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs](https://arxiv.org/pdf/2508.18264) <br> Sixun Dong, Juhua Hu, Mian Zhang, Ming Yin, Yanjie Fu, Qi Qian | 2025/08 | [GitHub](https://github.com/Ironieser/MMTok)<br>[Project Page](https://cv.ironieser.cc/projects/mmtok.html) | - | - |
| [![PDF](https://img.shields.io/badge/EMNLP-2025-blue)]() [![Star](https://img.shields.io/github/stars/zju-jiyicheng/SpecVLM.svg?style=social&label=Star)]()<br>[SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning](https://arxiv.org/pdf/2508.16201v1) <br> Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li | 2025/08 | [GitHub](https://github.com/zju-jiyicheng/SpecVLM) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]() [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](http://arxiv.org/abs/2508.13460v1) <br>[Revisiting MLLM Token Technology through the Lens of Classical Visual Coding](http://arxiv.org/abs/2508.13460v1) <br> Jinming Liu, Junyan Lin, Yuntao Wei, Kele Shao, Keda Tao, Jianguo Huang, Xudong Yang, Zhibo Chen, Huan Wang, Xin Jin | 2025/08 | - | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&label=Star)]()<br>[InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency](https://arxiv.org/pdf/2508.18265) <br> Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, et al. | 2025/08 | [GitHub](https://github.com/OpenGVLab/InternVL)<br>[Model](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](http://arxiv.org/abs/2508.10552v1) <br>[When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](http://arxiv.org/abs/2508.10552v1)<br> Huyu Wu, Meng Tang, Xinhan Zheng, Haiyun Jiang | 2025/08 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/ModelTC/LightCompress.svg?style=social&label=Star)]()<br>[LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit](https://www.arxiv.org/pdf/2508.09981) <br> Chengtao Lv, Bilang Zhang, Yang Yong, Ruihao Gong, Yushi Huang, Shiqiao Gu, Jiajun Wu, Yumeng Shi, Jinyang Guo, Wenya Wang | 2025/08 | [GitHub](https://github.com/ModelTC/LightCompress)<br> | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() | - |
| [![KDD](https://img.shields.io/badge/KDD-2025-blue)]() <br>[ADMIRE: ADaptive method to enhance Multiple Image REsolutions in text-rich multi-image understanding](https://dl.acm.org/doi/pdf/10.1145/3711896.3737187) <br> Qipeng Zhu, Xiong Wang, Zhihong Lu, Jiangwei Lao, Congyun Jin, Jie Chen, Yingzhe Peng, Qi Zhu, Lianzhen Zhong, Jiajia Liu, Peng Wei, Jian Wang| 2025/08 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]() [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning](https://arxiv.org/pdf/2508.07871v1)<br> Yanshu Li, Jianjiang Yang, Zhennan Shen, Ligong Han, Haoyan Xu, Ruixiang Tang | 2025/08 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](http://arxiv.org/abs/2508.06084v1)<br> Weichen Zhang, Zhui Zhu, Ningbo Li, Kebin Liu, Yunhao Liu | 2025/08 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](http://arxiv.org/abs/2508.06038v1) <br> Huanyu Wang, Jushi Kai, Haoli Bai, Lu Hou, Bo Jiang, Ziwei He, Zhouhan Lin | 2025/08 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/sihany077/VFlowOpt.svg?style=social&label=Star)](https://github.com/sihany077/VFlowOpt)<br>[VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/abs/2508.05211)<br>Sihan Yang, Runsen Xu, Chenhang Cui, Tai Wang, Dahua Lin, Jiangmiao Pang | 2025/08 | [GitHub](https://github.com/sihany077/VFlowOpt)<br> | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]()<br>[![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br>[![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br>[![Approach](https://img.shields.io/badge/Retrain-yellow)]() [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br>[![Speed](https://img.shields.io/badge/Speed_Infer-orange)]()<br>[![Ratio](https://img.shields.io/badge/Dynamic-pink)]()<br>[![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/HVision-NKU/GlimpsePrune.svg?style=social&label=Star)]()<br>[A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2508.01548)<br>Quan-Sheng Zeng, Yunheng Li, Qilong Wang, Peng-Tao Jiang, Zuxuan Wu, Ming-Ming Cheng, Qibin Hou | 2025/08 | [GitHub](https://github.com/HVision-NKU/GlimpsePrune)<br>[Model](https://huggingface.co/collections/ashun989/glimpseprune) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/Danielement321/HiPrune.svg?style=social&label=Star)]()<br>[HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/pdf/2508.00553) <br> Jizhihui Liu, Feiyi Du, Guangdao Zhu, Niu Lian, Jun Li, Bin Chen | 2025/08 | [GitHub](https://github.com/Danielement321/HiPrune) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/cokeshao/Awesome-Multimodal-Token-Compression.svg?style=social&label=Star)]()<br>[When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression](https://arxiv.org/abs/2507.20198) <br> Kele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang Sui, Huan Wang | 2025/07 | [GitHub](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression) | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/pdf/2507.09702) <br> Phat Nguyen, Ngai-Man Cheung | 2025/07 | - | - | - |
| [![ACM MM](https://img.shields.io/badge/ACMMM-2025-blue)]() [![Star](https://img.shields.io/github/stars/ASGO-MM/Short-LVLM.svg?style=social&label=Star)]()<br>[Short-LVLM: Compressing and Accelerating Large Vision-Language Models by Pruning Redundant Layers](https://arxiv.org/pdf/2507.23362) <br> Ji Ma, Wei Suo, Peng Wang, Yanning Zhang | 2025/07 | [GitHub](https://github.com/ASGO-MM/Short-LVLM)<br> | [![Area](https://img.shields.io/badge/Image-purple)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/liaolea/TransPrune.svg?style=social&label=Star)]()<br>[TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model](https://arxiv.org/abs/2507.20630v1) <br> Ao Li, Yuxiang Duan, Jinghui Zhang, Congbo Ma, Yutong Xie, Gustavo Carneiro, Mohammad Yaqub, Hu Wang | 2025/07 | [GitHub](https://github.com/liaolea/TransPrune)<br>| [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/YuchenLiu98/METEOR.svg?style=social&label=Star)]()<br>[METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models](https://arxiv.org/abs/2507.20842v1)<br> Yuchen Liu, Yaoming Wang, Bowen Shi, Xiaopeng Zhang, Wenrui Dai, Chenglin Li, Hongkai Xiong, Qi Tian | 2025/07 | [GitHub](https://github.com/YuchenLiu98/METEOR)<br> | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]()<br>[Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.20749v1) <br> Yiran Huang, Lukas Thede, Massimiliano Mancini, Wenjia Xu, Zeynep Akata | 2025/07 | - | [![Area](https://img.shields.io/badge/Image-purple)]() | [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Re--train-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/HYUNJS/STTM.svg?style=social&label=Star)]() <br>[Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs](https://arxiv.org/pdf/2507.07990) <br> Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim | 2025/07 | [GitHub](https://github.com/HYUNJS/STTM)<br>[Project Page](https://www.jshyun.me/projects/sttm) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2507.13348) [![Star](https://img.shields.io/github/stars/dvlab-research/VisionThink.svg?style=social&label=Star)]()<br>[VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning](https://arxiv.org/pdf/2507.13348)<br> Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia | 2025/07 | [GitHub](https://github.com/dvlab-research/VisionThink)<br>[Model](https://huggingface.co/collections/Senqiao/visionthink) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2507.15428) <br>[EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/pdf/2507.15428)<br>Jiaao Li, Kaiyuan Li, Chen Gao, Yong Li, Xinlei Chen | 2025/07 | -| [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/zxcvfd13502/GSOP.svg?style=social&label=Star)]()<br>[Beyond Token Pruning: Operation Pruning in Vision-Language Models](https://arxiv.org/pdf/2507.02909) <br> Aoming Liu, Reuben Tan, Boqing Gong, Bryan A. Plummer | 2025/07 | [GitHub](https://github.com/zxcvfd13502/GSOP) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/pdf/2507.02279) <br> Juntao Liu, Liqiang Niu, Wenchao Chen, Jie Zhou, Fandong Meng | 2025/07 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![NeurIPS](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/Theia-4869/CDPruner.svg?style=social&label=Star)]()<br>[Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs](https://arxiv.org/pdf/2506.10967) <br> Qizhe Zhang, Mengzhen Liu, Lichen Li, Ming Lu, Yuan Zhang, Junwen Pan, Qi She, Shanghang Zhang | 2025/06 | [GitHub](https://github.com/Theia-4869/CDPruner)<br>[Project Page](https://theia4869.com/CDPruner) | [![Image](https://img.shields.io/badge/Image-purple)]() [![Video](https://img.shields.io/badge/Video-purple)]() | - |
| [![NeurIPS](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/yunzhuzhang0918/flexselect.svg?style=social&label=Star)]()<br>[FlexSelect: Flexible Token Selection for Efficient Long Video Understanding](https://arxiv.org/pdf/2506.00993) <br> Yunzhu Zhang, Yu Lu, Tianyi Wang, Fengyun Rao, Yi Yang, Linchao Zhu | 2025/06 | [GitHub](https://github.com/yunzhuzhang0918/flexselect)<br> [Project Page](https://yunzhuzhang0918.github.io/flex_select) <br> [Model](https://huggingface.co/yunzhuyunzhu) | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/VectorSpaceLab/Video-XL.svg?style=social&label=Star)]()<br>[Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV Sparsification](https://arxiv.org/pdf/2506.19225)<br> Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu | 2025/06 | [GitHub](https://github.com/VectorSpaceLab/Video-XL)<br> | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]()  | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment](https://arxiv.org/abs/2506.22283) <br> Rui Xu, Yunke Wang, Yong Luo, Bo Du | 2025/06 | - | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoderr-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]()<br>[Generic Token Compression in Multimodal Large Language Models from an Explainability Perspective](https://arxiv.org/pdf/2506.01097) <br> Lei Lei, Jie Gu, Xiaokang Ma, Chu Tang, Jingmin Chen, Tong Xu | 2025/06 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/CR400AF-A/SparseMM.svg?style=social&label=Star)]() <br>[SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs](https://arxiv.org/pdf/2506.05344) <br> Jiahui Wang, Zuyan Liu, Yongming Rao, Jiwen Lu | 2025/06 | [GitHub](https://github.com/CR400AF-A/SparseMM)<br> [Project Page](https://cr400af-a.github.io/SparseMM/) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/IVGSZ/Flash-VStream.svg?style=social&label=Star)]()<br>[Flash-VStream: Efficient Real-Time Understanding for Long Video Streams](https://arxiv.org/pdf/2506.23825)<br> Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Xiaojie Jin | 2025/06 | [GitHub](https://github.com/IVGSZ/Flash-VStream)<br> [Project Page](https://zhang9302002.github.io/vstream-iccv-page/) <br> [Model](https://huggingface.co/zhang9302002/Flash-VStream-Qwen-7b) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2506.21862) [![Star](https://img.shields.io/github/stars/HumanMLLM/LLaVA-Scissor.svg?style=social&label=Star)]()<br>[LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/pdf/2506.21862) <br> Boyuan Sun, Jiaxing Zhao, Xihan Wei, Qibin Hou | 2025/06 | [GitHub](https://github.com/HumanMLLM/LLaVA-Scissor)<br> [Model](https://huggingface.co/BBBBCHAN/LLaVA-Scissor-baseline-7B) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[GreedyPrune: Retenting Critical Visual Token Set for Large Vision Language Models](https://arxiv.org/pdf/2506.13166) <br> Ruiguang Pei, Weiqing Sun, Zhihui Fu, Jun Wang | 2025/06 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/abs/2506.07138) [![Star](https://img.shields.io/github/stars/visresearch/LLaVA-STF.svg?style=social&label=Star)]()<br>[Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)<br> Hao Tang, Chengchao Shen | 2025/06 | [GitHub](https://github.com/visresearch/LLaVA-STF)<br> [Model](https://huggingface.co/visresearch/LLaVA-STF/tree/main)  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2506.03990) <br>[DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/pdf/2506.03990)<br> Hongzhi Zhang, Jingyuan Zhang, Xingguang Ji, Qi Wang, Fuzheng Zhang | 2025/06 | - | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/ZLKong/Awesome-Collection-Token-Reduction.svg?style=social&label=Star)]()<br>[Token Reduction Should Go Beyond Efficiency in Generative Models ‚Äì From Vision, Language to Multimodality](https://arxiv.org/pdf/2505.18227) <br> Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, Marinka Zitnik | 2025/05 | [GitHub](https://github.com/ZLKong/Awesome-Collection-Token-Reduction)<br>  | - | - |
| [![ICML](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/wangqinsi1/2025-ICML-CoreMatching.svg?style=social&label=Star)]()<br>[Corematching: A co-adaptive sparse inference framework with token and neuron pruning for comprehensive acceleration of vision-language models](https://arxiv.org/pdf/2505.19235) <br> Qinsi Wang, Hancheng Ye, Ming-Yu Chung, Yudong Liu, Yueqian Lin, Martin Kuo, Mingyuan Ma, Jianyi Zhang, Yiran Chen | 2025/05 | [GitHub](https://github.com/wangqinsi1/2025-ICML-CoreMatching)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![NeurIPS](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/EmbodiedCity/NeurIPS2025-Balanced-Token-Pruning.svg?style=social&label=Star)]() <br>[Balanced token pruning: Accelerating vision language models beyond local optimization](https://arxiv.org/pdf/2505.22038) <br> Kaiyuan Li, Xiaoyue Chen, Chen Gao, Yong Li, Xinlei Chen | 2025/05 | [GitHub](https://github.com/EmbodiedCity/NeurIPS2025-Balanced-Token-Pruning)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[ToDRE: Visual Token Pruning via Diversity and Task Relevance for Multimodal LLMs](https://arxiv.org/pdf/2505.18757) <br> Duo Li, Zuhao Yang, Shijian Lu | 2025/05 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/abs/2505.07062) [![Star](https://img.shields.io/github/stars/ByteDance-Seed/Seed1.5-VL.svg?style=social&label=Star)]()<br>[Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062) <br> ByteDance Seed | 2025/05 | [GitHub](https://github.com/ByteDance-Seed/Seed1.5-VL)<br> [Demo](https://huggingface.co/spaces/ByteDance-Seed/Seed1.5-VL) <br>[Homepage](https://seed.bytedance.com/zh/tech/seed1_5_vl) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() <br> [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/cokeshao/HoliTom.svg?style=social&label=Star)]()<br>[HoliTom: Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/pdf/2505.21334) <br> Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang | 2025/05 | [GitHub](https://github.com/cokeshao/HoliTom)<br> [Project Page](https://cokeshao.github.io/HoliTom_Web/) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/Tencent/SelfEvolvingAgent.svg?style=social&label=Star)]()<br>[VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/pdf/2505.22654) <br> Ce Zhang, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia Sycara, Haitao Mi, Dong Yu | 2025/05 | [GitHub](https://github.com/Tencent/SelfEvolvingAgent/tree/main/VScan)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]()  | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[AdaTP: Attention-Debiased Token Pruning for Video Large Language Models](https://arxiv.org/pdf/2505.20100) <br> Fengyuan Sun, Leqi Shen, Hui Chen, Sicheng Zhao, Jungong Han, Guiguang Ding | 2025/05 | - | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/shilinyan99/CrossLMM.svg?style=social&label=Star)]()<br>[CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms](https://arxiv.org/pdf/2505.17020) <br> Shilin Yan, Jiaming Han, Joey Tsai, Hongwei Xue, Rongyao Fang, Lingyi Hong, Ziyu Guo, Ray Zhang | 2025/05 | [GitHub](https://github.com/shilinyan99/CrossLMM)<br> | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[Clapper: Compact Learning and Video Representation in VLMs](https://arxiv.org/pdf/2505.15529) <br> Lingyu Kong, Hongzhi Zhang, Jingyuan Zhang, Jianzhao Huang, Kunze Li, Qi Wang, Fuzheng Zhang | 2025/05 | - | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoderr-cyan)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/xuyang-liu16/VidCom2.svg?style=social&label=Star)]()<br>[Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models](https://arxiv.org/pdf/2505.14454) <br> Xuyang Liu, Yiyu Wang, Junpeng Ma, Linfeng Zhang | 2025/05 | [GitHub](https://github.com/xuyang-liu16/VidCom2)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning](https://arxiv.org/pdf/2505.11945) <br> Bonan li, Zicheng Zhang, Songhua Liu, Weihao Yu, Xinchao Wang | 2025/05 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br>[![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br>[![Approach](https://img.shields.io/badge/Re--train-yellow)]()<br>[![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![NeurIPS](https://img.shields.io/badge/NeurIPS-2025-blue)]() <br>[Why 1 + 1 < 1 in Visual Token Pruning: Beyond Na√Øve Integration via Multi-Objective Balanced Covering](https://arxiv.org/pdf/2505.10118) <br> Yangfu Li, Hongjian Zhan, Tianyi Chen, Qi Liu, Yue Lu | 2025/05 | - | - | - |
| [![ECAI](https://img.shields.io/badge/ECAI-2025-blue)]() <br>[Lossless Token Merging Even Without Fine-Tuning in Vision Transformers](https://arxiv.org/pdf/2505.15160) <br> Jaeyeon Lee, Dong-Wan Choi | 2025/05 | - | [![Stage](https://img.shields.io/badge/ViT-cyan)]() | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding](https://arxiv.org/pdf/2504.20384) <br> Yanan Guo, Wenhui Dong, Jun Song, Shiding Zhu, Xuan Zhang, Hanqing Yang, Yingbo Wang, Yang Du, Xianing Chen, Bo Zheng | 2025/04 | - | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![ACL+2025+findings](https://img.shields.io/badge/ACL_Findings-2025-blue)]() [![Star](https://img.shields.io/github/stars/G-JWLee/TAMP.svg?style=social&label=Star)]()<br>[TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models](https://arxiv.org/pdf/2504.09897)<br> Jaewoo Lee, Keyang Xuan, Chanakya Ekbote, Sandeep Polisetty, Yi R. Fung, Paul Pu Liang | 2025/04 | [GitHub](https://github.com/G-JWLee/TAMP)<br>  | - | [![Stage](https://img.shields.io/badge/LLM-cyan)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]()<br>[Token Sequence Compression for Efficient Multimodal Computing](https://arxiv.org/abs/2504.17892) <br> Yasmine Omri, Parth Shroff, Thierry Tambe | 2025/04 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/RainBowLuoCS/VCM.svg?style=social&label=Star)]()<br>[VCM: Vision Concept Modeling with Adaptive Vision Token Compression via Instruction Fine-Tuning](https://arxiv.org/pdf/2504.19627) <br> Run Luo, Renke Shan, Longze Chen, Ziqiang Liu, Lu Wang, Min Yang, Xiaobo Xia | 2025/04 | [GitHub](https://github.com/RainBowLuoCS/VCM)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![ACMMM](https://img.shields.io/badge/ACMMM-2025-blue)]()  [![Star](https://img.shields.io/github/stars/yaolinli/TimeChat-Online.svg?style=social&label=Star)]()<br>[TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/pdf/2504.17343) <br> Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, Lingpeng Kong, Qi Liu, Yuanxing Zhang, Xu Sun | 2025/04 | [GitHub](https://github.com/yaolinli/TimeChat-Online)<br> [Project Page](https://timechat-online.github.io/) <br> [Dataset](https://huggingface.co/datasets/yaolily/TimeChat-Online-139K) <br> [Model](https://huggingface.co/wyccccc/TimeChatOnline-7B) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]() [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2504.17040) [![Star](https://img.shields.io/github/stars/MikeWangWZHL/dymu.svg?style=social&label=Star)]()<br>[DYMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/pdf/2504.17040)<br> Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, Ran Xu | 2025/04 | [GitHub](https://github.com/MikeWangWZHL/dymu)<br> [Project Page](https://mikewangwzhl.github.io/dymu) [Model](https://huggingface.co/mikewang/DyMU/tree/main) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/quicksviewer/quicksviewer.svg?style=social&label=Star)]()<br>[Quicksviewer: An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](https://arxiv.org/pdf/2504.15270) <br> Ji Qi, Yuan Yao, Yushi Bai, Bin Xu, Juanzi Li, Zhiyuan Liu, Tat-Seng Chua | 2025/04 | [GitHub](https://github.com/quicksviewer/quicksviewer)<br> [Project Page](https://quicksviewer.github.io) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/orailix/PACT.svg?style=social&label=Star)]()<br>[PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models](https://arxiv.org/pdf/2504.08966)<br> Mohamed Dhouib, Davide Buscaldi, Sonia Vanier, Aymen Shabou | 2025/04 | [GitHub](https://github.com/orailix/PACT)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br>[![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br>[![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br>[![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br>[![Speed](https://img.shields.io/badge/Speed_Infer-orange)]()<br>[![Ratio](https://img.shields.io/badge/Fix-pink)]()<br>[![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]()<br>[QG-VTC: Question-Guided Visual Token Compression in MLLMs for Efficient VQA](https://arxiv.org/pdf/2504.00654) <br> Shuai Li, Jian Xu, Xiao-Hui Li, Chao Deng, Lin-Lin Huang | 2025/04 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[Long-VMNet: Accelerating Long-Form Video Understanding via Fixed Memory](https://arxiv.org/pdf/2503.13707) <br> Saket Gurukar, Asim Kadav | 2025/03 | - | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2503.10501) [![Star](https://img.shields.io/github/stars/ShawnTan86/TokenCarve.svg?style=social&label=Star)]()<br>[Tokencarve: Information-preserving visual token compression in multimodal large language models](https://arxiv.org/pdf/2503.10501) <br> Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, Yaoxin Yang, Lin Zhang, Dongzhan Zhou, Tao Chen | 2025/03 | [GitHub](https://github.com/ShawnTan86/TokenCarve)<br>  | - | - |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/zwl666666/Skip-Vision.svg?style=social&label=Star)]()<br>[Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping](https://arxiv.org/pdf/2503.21817) <br> Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan | 2025/03 | [GitHub](https://github.com/zwl666666/Skip-Vision)<br> | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoderr-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() <br>[TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model](https://arxiv.org/pdf/2503.18278)<br> Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan | 2025/03 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2503.21307) [![Star](https://img.shields.io/github/stars/ludc506/InternVL-X.svg?style=social&label=Star)]()<br>[InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](https://arxiv.org/pdf/2503.21307) <br> Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, Huo Cao | 2025/03 | [GitHub](https://github.com/ludc506/InternVL-X)<br> [Model](https://huggingface.co/LLCC506) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/Hai-chao-Zhang/VQToken.svg?style=social&label=Star)]()<br>[Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Model](https://arxiv.org/pdf/2503.16980) <br>  Haichao Zhang,  Yun Fu | 2025/03 | [GitHub](https://github.com/Hai-chao-Zhang/VQToken)<br> [Project Page](https://www.zhanghaichao.xyz/VQToken/) <br> [Model](https://huggingface.co/haichaozhang/VQ-Token-llava-ov-0.5b) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/lntzm/HICom.svg?style=social&label=Star)]()<br>[HICom: Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models](https://arxiv.org/pdf/2503.16036)<br> Zhihang Liu, Chen-Wei Xie, Pandeng Li, Liming Zhao, Longxiang Tang, Yun Zheng, Chuanbin Liu, Hongtao Xie | 2025/03 | [GitHub](https://github.com/lntzm/HICom)<br> [Model](https://huggingface.co/lntzm/HICom_7B_qwen25_directg_local43_global32) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![NeurIPS](https://img.shields.io/badge/NeurIPS-2025-blue)]()  [![Star](https://img.shields.io/github/stars/LunarShen/FastVID.svg?style=social&label=Star)]()<br>[FastVID: Dynamic Density Pruning for Fast Video Large Language Models](https://arxiv.org/abs/2503.11187)<br> Leqi Shen, Guoqiang Gong, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Guiguang Ding | 2025/03 | [GitHub](https://github.com/LunarShen/FastVID) <br> | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/ArmenJeddi/saint.svg?style=social&label=Star)]()<br>[Similarity-Aware Token Pruning: Your VLM but Faster](https://arxiv.org/pdf/2503.11549) <br> Ahmadreza Jeddi, Negin Baghbanzadeh, Elham Dolatabadi, Babak Taati | 2025/03 | [GitHub](https://github.com/ArmenJeddi/saint)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]()  | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/bzluan/AdaptPrune.svg?style=social&label=Star)]()<br>[Multi-Cue Adaptive Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2503.08019)<br> Bozhi Luan, Wengang Zhou, Hao Feng, Zhe Wang, Xiaosong Li, Houqiang Li | 2025/03 | [GitHub](https://github.com/bzluan/AdaptPrune)<br>  | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/VectorSpaceLab/Video-XL.svg?style=social&label=Star)]()<br>[Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding](https://arxiv.org/pdf/2503.18478) <br> Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, Bo Zhao | 2025/03 | [GitHub](https://github.com/VectorSpaceLab/Video-XL/tree/main/Video-XL-Pro)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoderr-cyan)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[STORM: Token-Efficient Long Video Understanding for Multimodal LLMs](https://arxiv.org/pdf/2503.04130)<br> Jindong Jiang, Xiuyu Li, Zhijian Liu, Muyang Li, Guo Chen, Zhiqi Li, De-An Huang, Guilin Liu, Zhiding Yu, Kurt Keutzer, Sungjin Ahn, Jan Kautz, Hongxu Yin, Yao Lu, Song Han, Wonmin Byeon | 2025/03 | [Project Page](https://research.nvidia.com/labs/lpr/storm) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br>[![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br>[![Ratio](https://img.shields.io/badge/Fix-pink)]()<br>[![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/vbdi/divprune.svg?style=social&label=Star)]()<br>[DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models](https://arxiv.org/pdf/2503.02175)<br> Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, Yong Zhang | 2025/03 | [GitHub](https://github.com/vbdi/divprune)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/VisionXLab/LRS-VQA.svg?style=social&label=Star)]()<br>[When LVLM Meets Large RS Imagery: Coarse-to-Fine Text-Guided Token Pruning](https://arxiv.org/pdf/2503.07588) <br> Junwei Luo, Yingying Zhang, Xue Yang, Kang Wu, Qi Zhu, Lei Liang, Jingdong Chen, Yansheng Li | 2025/03 | [GitHub](https://github.com/VisionXLab/LRS-VQA)<br> [Dataset](https://huggingface.co/datasets/ll-13/LRS-VQA) | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[Silent Hazards of Token Reduction in Vision-Language Models](https://arxiv.org/pdf/2503.06794) <br> Yizheng Sun, Hao Li, Chang Xu, Hongpeng Zhou, Chenghua Lin, Riza Batista-Navarro, Jingyuan Sun | 2025/03 | - | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/NUST-Machine-Intelligence-Laboratory/prune_and_merge.svg?style=social&label=Star)]()<br>[Prune and Merge: Efficient Token Compression For Vision Transformer With Spatial Information Preserved](https://arxiv.org/pdf/2503.23455) <br> Junzhu Mao, Yang Shen, Jinyang Guo, Yazhou Yao, Xiansheng Hua | 2025/03 | [GitHub](https://github.com/NUST-Machine-Intelligence-Laboratory/prune_and_merge)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models](https://arxiv.org/pdf/2502.14504) <br> Yu Meng, Kaiyuan Li, Chenran Huang, Chen Gao, Xinlei Chen, Yong Li, Xiaoping Zhang | 2025/02 | - | - | - |
| [![EMNLP](https://img.shields.io/badge/EMNLP-2025-blue)]() [![Star](https://img.shields.io/github/stars/ZichenWen1/DART.svg?style=social&label=Star)]() <br>[Stop Looking for Important Tokens in Multimodal Language Models for Token Pruning](https://arxiv.org/pdf/2502.11494) <br> Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, Linfeng Zhang | 2025/02 | [GitHub](https://github.com/ZichenWen1/DART)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL.svg?style=social&label=Star)]()<br>[Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923) <br> QwenTeam | 2025/02 | [GitHub](https://github.com/QwenLM/Qwen2.5-VL)<br> [Model](https://huggingface.co/Qwen) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Re--train-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() <br>[FCoT-VL: Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression](https://arxiv.org/pdf/2502.18512) <br> Jianjian Li, Junquan Fan, Feng Tang, Gang Huang, Shitao Zhu, Songlin Liu, Nian Xie, Wulong Liu, Yong Liao | 2025/02 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![ACL Findings](https://img.shields.io/badge/ACL_Findings-2025-blue)]() <br>[Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?](https://arxiv.org/pdf/2502.11501) <br> Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, Linfeng Zhang | 2025/02 | - | - | - |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/anakin-skywalker-Joseph/Folder.svg?style=social&label=Star)]()<br>[FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance](https://arxiv.org/pdf/2501.02430) <br> Haicheng Wang, Zhemeng Yu, Gabriele Spadaro, Chen Ju, Victor Qu√©tu, Shuai Xiao, Enzo Tartaglione | 2025/01 | [GitHub](https://github.com/anakin-skywalker-Joseph/Folder)<br>  | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/xuyang-liu16/GlobalCom2.svg?style=social&label=Star)]()<br>[Compression with Global Guidance: Towards Training-Free High-Resolution MLLMs Acceleration](https://arxiv.org/pdf/2501.05179v1) <br> Xuyang Liu, Ziming Wang, Yuhang Han, Yingyao Wang, Jiale Yuan, Jun Song, Bo Zheng, Linfeng Zhang, Siteng Huang, Honggang Chen | 2025/01 | [GitHub](https://github.com/xuyang-liu16/GlobalCom2)<br>  | - | - |
| [![AAAI](https://img.shields.io/badge/AAAI-2025-blue)]() [![Star](https://img.shields.io/github/stars/jytmelon/G-Prune.svg?style=social&label=Star)]() <br>[What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph](https://arxiv.org/pdf/2501.02268) <br> Yutao Jiang, Qiong Wu, Wenhao Lin, Wei Yu, Yiyi Zhou | 2025/01 | [GitHub](https://github.com/jytmelon/G-Prune)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/JiuTian-VL/FALCON.svg?style=social&label=Star)]()<br>[FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal LLMs via Visual Registers](https://arxiv.org/abs/2501.16297)<br> Renshan Zhang, Rui Shao, Gongwei Chen, Miao Zhang, Kaiwen Zhou, Weili Guan, Liqiang Nie | 2025/01 | [GitHub](https://github.com/JiuTian-VL/FALCON)<br> [Project Page](https://jiutian-vl.github.io/FALCON.github.io/) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/L-Hugh/RedundancyLens.svg?style=social&label=Star)]()<br>[Beyond Token Compression: A Training-Free Reduction Framework for Efficient Visual Processing in MLLMs](https://arxiv.org/abs/2501.19036v1) <br> Hongliang Li, Jiaxin Zhang, Wenhui Liao, Dezhi Peng, Kai Ding, Lianwen Jin | 2025/01 | [GitHub](https://github.com/L-Hugh/RedundancyLens)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/username/repo.svg?style=social&label=Star)]()<br>[DyRate: Dynamic Token Reduction during Generation for Vision Language Models](https://arxiv.org/pdf/2501.14204) <br> Xiaoyu Liang, Chaofeng Guan, Jiaying Lu, Huiyao Chen, Huan Wang, Haoji Hu | 2025/01 | [GitHub](https://github.com/your-repo-link)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![NAACL+2025+findings](https://img.shields.io/badge/NAACL_findings-2025-blue)]() <br>[LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning Approach for Multi-modal Large Language Models](https://arxiv.org/abs/2501.13652)<br> Yizheng Sun, Yanze Xin, Hao Li, Jingyuan Sun, Chenghua Lin, Riza Batista-Navarro | 2025/01 | - | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2501.13106) [![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA3.svg?style=social&label=Star)]()<br>[VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding](https://arxiv.org/pdf/2501.13106) <br> Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, Deli Zhao | 2025/01 | [GitHub](https://github.com/DAMO-NLP-SG/VideoLLaMA3)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2501.09532) <br>[AdaFV: Rethinking of Visual-Language alignment for VLM acceleration](https://arxiv.org/pdf/2501.09532)<br> Jiayi Han, Liang Du, Yiwen Wu, Xiangguo Zhou, Hongwei Du, Weibo Zheng | 2025/01 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)]() [![Star](https://img.shields.io/github/stars/xuyang-liu16/GlobalCom2.svg?style=social&label=Star)]()<br>[Global Compression Commander: Plug-and-Play Inference Acceleration for High-Resolution Large Vision-Language Models](https://arxiv.org/pdf/2501.05179) <br> Xuyang Liu, Ziming Wang, Junjie Chen, Yuhang Han, Yingyao Wang, Jiale Yuan, Jun Song, Linfeng Zhang, Siteng Huang, Honggang Chen | 2025/01 | [GitHub](https://github.com/xuyang-liu16/GlobalCom2)<br>  | - | - |
| [![ICLR](https://img.shields.io/badge/ICLR-2025-blue)]() [![Star](https://img.shields.io/github/stars/ictnlp/LLaVA-Mini.svg?style=social&label=Star)]()<br>[LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://arxiv.org/pdf/2501.03895) <br> Shaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng | 2025/01 | [GitHub](https://github.com/ictnlp/LLaVA-Mini)<br> [Model](https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/thu-nics/FrameFusion.svg?style=social&label=Star)]()<br>[FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models](https://arxiv.org/pdf/2501.01986) <br> Tianyu Fu, Tengxuan Liu, Qinghao Han, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, Yu Wang | 2025/01 | [GitHub](https://github.com/thu-nics/FrameFusion)<br> [Project Page](https://thu-nics.github.io/FrameFusion_Project_Page) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2025-red)](https://arxiv.org/pdf/2501.00574) [![Star](https://img.shields.io/github/stars/OpenGVLab/VideoChat-Flash.svg?style=social&label=Star)]()<br>[VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling](https://arxiv.org/pdf/2501.00574) <br> Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, Limin Wang | 2025/01 | [GitHub](https://github.com/OpenGVLab/VideoChat-Flash)<br> [Demo](https://github.com/OpenGVLab/VideoChat-Flash/blob/main) <br> [Model](https://huggingface.co/collections/OpenGVLab/videochat-flash) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoderr-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)](https://arxiv.org/pdf/2412.05185) [![Star](https://img.shields.io/github/stars/gls0425/LinVT.svg?style=social&label=Star)]()<br>[LinVT: Empower Your Image-level Large Language Model to Understand Videos](https://arxiv.org/pdf/2412.05185)<br> Lishuai Gao, Yujie Zhong, Yingsen Zeng, Haoxian Tan, Dengjie Li, Zheng Zhao | 2024/12 | [GitHub](https://github.com/gls0425/LinVT)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() <br>[Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction](https://arxiv.org/pdf/2412.00556)<br> Shiyu Zhao, Zhenting Wang, Felix Juefei-Xu, Xide Xia, Miao Liu, Xiaofang Wang, Mingfu Liang, Ning Zhang, Dimitris N. Metaxas, Licheng Yu | 2024/12 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![AAAI](https://img.shields.io/badge/AAAI-2025-blue)]() <br>[St3: Accelerating multimodal large language model by spatial-temporal visual token trimming](https://arxiv.org/pdf/2412.20105)<br> Jiedong Zhuang, Lu Lu, Ming Dai, Rui Hu, Jian Chen, Qiang Liu, Haoji Hu | 2024/12 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/hulianyuyy/iLLaVA.svg?style=social&label=Star)]()<br>[iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large Multimodal Models](https://arxiv.org/pdf/2412.06263) <br> Lianyu Hu, Fanhua Shang, Liang Wan, Wei Feng | 2024/12 | [GitHub](https://github.com/hulianyuyy/iLLaVA)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | - |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/markendo/FEATHER.svg?style=social&label=Star)]()<br>[Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration](https://arxiv.org/pdf/2412.13180) <br> Mark Endo, Xiaohan Wang, Serena Yeung-Levy | 2024/12 | [GitHub](https://github.com/markendo/FEATHER)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/Visual-AI/PruneVid.svg?style=social&label=Star)]()<br>[PruneVid: Visual Token Pruning for Efficient Video Large Language Models](https://arxiv.org/pdf/2412.16117v1)<br> Xiaohu Huang, Hao Zhou, Kai Han | 2024/12 | [GitHub](https://github.com/Visual-AI/PruneVid)<br> [Project Page](https://visual-ai.github.io/prunevid/) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)](https://arxiv.org/pdf/2412.20504) [![Star](https://img.shields.io/github/stars/SCZwangxiao/video-ReTaKe.svg?style=social&label=Star)]()<br>[RETAKE: Reducing Temporal and Knowledge Redundancy for Long Video Understanding](https://arxiv.org/pdf/2412.20504)<br> ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video Understanding | 2024/12 | [GitHub](https://github.com/SCZwangxiao/video-ReTaKe)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() | - |
| [![ICLR](https://img.shields.io/badge/ICLR-2025-blue)]() [![Star](https://img.shields.io/github/stars/Osilly/dynamic_llava.svg?style=social&label=Star)]() <br>[Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification.](https://arxiv.org/pdf/2412.00876) <br> Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaosheng Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, Yao Hu, Shaohui Lin | 2024/12 | [GitHub](https://github.com/Osilly/dynamic_llava) <br> [Model-7B](https://huggingface.co/Osilly/Dynamic-LLaVA-7B) <br> [Model-13B](https://huggingface.co/Osilly/Dynamic-LLaVA-13B) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/apple/ml-fastvlm.svg?style=social&label=Star)]()<br>[FastVLM: Efficient Vision Encoding for Vision Language Models](https://arxiv.org/pdf/2412.13303)<br> Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari | 2024/12 | [GitHub](https://github.com/apple/ml-fastvlm)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[LLaVA-Zip: Adaptive Visual Token Compression with Intrinsic Image Redundancy Modeling](https://arxiv.org/abs/2412.08771)<br> Ke Wang, Hong Xuan | 2024/12 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/OpenGVLab/PVC.svg?style=social&label=Star)]()<br>[PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models](https://arxiv.org/pdf/2412.09613) <br> Chenyu Yang, Xuan Dong, Xizhou Zhu, Weijie Su, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, Jifeng Dai | 2024/12 | [GitHub](https://github.com/OpenGVLab/PVC)<br> [Model](https://huggingface.co/OpenGVLab/PVC-InternVL2-8B) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/Hon-Wong/ByteVideoLLM.svg?style=social&label=Star)]()<br>[Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM](https://arxiv.org/pdf/2412.09530) <br> Han Wang, Yuxiang Nie, Yongjie Ye, Deng GuanYu, Yanjie Wang, Shuai Li, Haiyang Yu, Jinghui Lu, Can Huang | 2024/12 | [GitHub](https://github.com/Hon-Wong/ByteVideoLLM)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() | - |
| [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/LaVi-Lab/AIM.svg?style=social&label=Star)]()<br>[AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](https://arxiv.org/abs/2412.03248)<br> Yiwu Zhong, Zhuoming Liu, Yin Li, Liwei Wang | 2024/12 | [GitHub](https://github.com/LaVi-Lab/AIM)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br>[![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br>[![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br>[![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/VisionZip.svg?style=social&label=Star)]()<br>[VisionZip: Longer is Better but Not Necessary in Vision Language Models](https://arxiv.org/pdf/2412.04467) <br> Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, Jiaya Jia | 2024/12 | [GitHub](https://github.com/dvlab-research/VisionZip)<br> [Demo](https://huggingface.co/spaces/Senqiao/VisionZip) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/MCG-NJU/p-MoD.svg?style=social&label=Star)]()<br>[p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay](https://arxiv.org/pdf/2412.04449) <br> Jun Zhang, Desen Meng, Zhengming Zhang, Zhenpeng Huang, Tao Wu, Limin Wang | 2024/12 | [GitHub](https://github.com/MCG-NJU/p-MoD)<br> [Model](https://huggingface.co/collections/JungleGym/p-mod-67506ac52553d194c55782df) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/Theia-4869/VisPruner.svg?style=social&label=Star)]()<br>[Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs](https://arxiv.org/pdf/2412.01818v2) <br> Qizhe Zhang, Aosong Cheng, Ming Lu, Renrui Zhang, Zhiyong Zhuo, Jiajun Cao, Shaobo Guo, Qi She, Shanghang Zhang | 2024/12 | [GitHub](https://github.com/Theia-4869/VisPruner)<br> [Project Page](https://theia4869.com/VisPruner) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() <br>[ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models](https://arxiv.org/pdf/2412.00447)<br> Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, Yansong Tang | 2024/12 | [Project Page](https://yxxxb.github.io/ATP-LLaVA-page/) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() <br>[Accelerating multimodal large language models by searching optimal vision token reduction](https://arxiv.org/pdf/2412.00556) <br> Shiyu Zhao, Zhenting Wang, Felix Juefei-Xu, Xide Xia, Miao Liu, Xiaofang Wang, Mingfu Liang, Ning Zhang, Dimitris N. Metaxas, Licheng Yu | 2024/12 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[Token Cropr: Faster ViTs for Quite a Few Tasks](https://arxiv.org/pdf/2412.00965) <br> Benjamin Bergner, Christoph Lippert, Aravindh Mahendran | 2024/12 | -  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/kawhiiiileo/FiCoCo.svg?style=social&label=Star)]()<br>[Filter, Correlate, Compress: Training-Free Token Reduction for MLLM Acceleration](https://arxiv.org/pdf/2411.17686) <br> Yuhang Han, Xuyang Liu, Zihan Zhang, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, Siteng Huang | 2024/11 | [GitHub](https://github.com/kawhiiiileo/FiCoCo)<br> [Project Page](https://ficoco-accelerate.github.io) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() <br> [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/KD-TAO/DyCoke.svg?style=social&label=Star)]()<br>[DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models](https://arxiv.org/pdf/2411.15024)<br> Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang | 2024/11 | [GitHub](https://github.com/KD-TAO/DyCoke)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)](https://arxiv.org/pdf/2411.14228) <br> [FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression](https://arxiv.org/pdf/2411.14228) <br> Yuke Zhu, Chi Xie, Shuang Liang, Bo Zheng, Sheng Guo | 2024/11 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/liuting20/MustDrop.svg?style=social&label=Star)]()<br>[Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model](https://arxiv.org/pdf/2411.10803)<br> Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, Linfeng Zhang | 2024/11 | [GitHub](https://github.com/liuting20/MustDrop)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![NeurIPS](https://img.shields.io/badge/NeurIPS-2024-blue)]() [![Star](https://img.shields.io/github/stars/rccchoudhury/rlt.svg?style=social&label=Star)]()<br>[Don't Look Twice: Faster Video Transformers with Run-Length Tokenization](https://arxiv.org/pdf/2411.05222)<br> Rohan Choudhury, Guanglei Zhu, Sihan Liu, Koichiro Niinuma, Kris M. Kitani, L√°szl√≥ Jeni | 2024/11 | [GitHub](https://github.com/rccchoudhury/rlt)<br> [Project Page](https://rccchoudhury.github.io/rlt) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![EMNLP Findings](https://img.shields.io/badge/EMNLP_Findings-2024-blue)]() <br>[Query-based Cross-Modal Projector Bolstering Mamba Multimodal LLM](https://aclanthology.org/2024.findings-emnlp.827.pdf)<br> SooHwan EomÔºå JayShim, GwanhyeongKoo, HaebinNa, Mark A. Hasegawa-Johnson, Sungwoong Kim, Chang D. Yoo | 2024/11 | - | [![Stage](https://img.shields.io/badge/Projector-cyan)]() | - |
|  [![ICLR](https://img.shields.io/badge/ICLR-2025-blue)]() [![Star](https://img.shields.io/github/stars/locuslab/llava-token-compression.svg?style=social&label=Star)]()<br>[Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters](https://arxiv.org/pdf/2411.03312)<br> Kevin Y. Li, Sachin Goyal, Joao D. Semedo, J. Zico Kolter | 2024/11 | [GitHub](https://github.com/locuslab/llava-token-compression)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![Publish](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/DoubtedSteam/DyVTE.svg?style=social&label=Star)]()<br>[Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and Empirical Findings](https://arxiv.org/abs/2411.19628)<br> Qiong Wu, Wenhao Lin, Yiyi Zhou, Weihao Ye, Zhanpeng Zen, Xiaoshuai Sun, Rongrong Ji | 2024/11 | [GitHub](https://github.com/DoubtedSteam/DyVTE)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[freePruner: A Training-free Approach for Large Multimodal Model Acceleration](https://arxiv.org/abs/2411.15446)<br> Bingxin Xu, Yuzhang Shang, Yunhao Ge, Qian Lou, Yan Yan | 2024/11 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[FoPru: Focal Pruning for Efficient Large Vision-Language Models](https://arxiv.org/abs/2411.14164) <br> Lei Jiang, Weizhe Huang, Tongxuan Liu, Yuting Zeng, Jing Li, Lechao Cheng, Xiaohua Xu | 2024/11 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[Efficient Multi-modal Large Language Models via Visual Token Grouping](https://arxiv.org/pdf/2411.17773?)<br> Minbin Huang, Runhui Huang, Han Shi, Yimeng Chen, Chuanyang Zheng, Xiangguo Sun, Xin Jiang, Zhenguo Li, Hong Cheng | 2024/11 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs](https://arxiv.org/pdf/2410.16267) <br> Michael S. Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Jongwoo Park, Kanchana Ranasinghe, Silvio Savarese, Ran Xu, Caiming Xiong, Juan Carlos Niebles | 2024/10 | - | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![ICLR](https://img.shields.io/badge/ICLR-2025-blue)]() <br>[ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification](https://arxiv.org/pdf/2410.08584)<br> Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, Bohan Zhuang | 2024/10 | - | [![Area](https://img.shields.io/badge/Image-purple)]() | [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() |
| [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/Jo-wang/TCA.svg?style=social&label=Star)]()<br>[Is Less More? Exploring Token Condensation as Training-free Test-time Adaptation](https://arxiv.org/pdf/2410.14729) <br> Zixin Wang, Dong Gong, Sen Wang, Zi Huang, Yadan Luo | 2024/10 | [GitHub](https://github.com/Jo-wang/TCA)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star)]()<br>[LLaVA-Video: Video Instruction Tuning With Synthetic Data](https://arxiv.org/pdf/2410.02713) <br> Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, Chunyuan Li | 2024/10 | [GitHub](https://github.com/LLaVA-VL/LLaVA-NeXT)<br> [Project Page](https://llava-vl.github.io/blog/2024-09-30-llava-video/) [Model](https://huggingface.co/collections/lmms-lab/llava-video) | [![Area](https://img.shields.io/badge/Video-purple)]() | - |
| [![NeurIPS](https://img.shields.io/badge/NeurIPS-2024-blue)]() <br>[Video Token Merging for Long-form Video Understanding](https://arxiv.org/pdf/2410.23782)<br> Seon-Ho Lee, Jue Wang, Zhikang Zhang, David Fan, Xinyu Li | 2024/10 | - | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![ICML](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/Vision-CAIR/LongVU.svg?style=social&label=Star)]()<br>[LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/pdf/2410.17434)<br> Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, Vikas Chandra | 2024/10 | [GitHub](https://github.com/Vision-CAIR/LongVU)<br> [Project Page](https://vision-cair.github.io/LongVU/) <br> [Model](https://huggingface.co/collections/Vision-CAIR/longvu) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoderr-cyan)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/Cooperx521/PyramidDrop.svg?style=social&label=Star)]()<br>[PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction](https://arxiv.org/pdf/2410.17247)<br> Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, Dahua Lin | 2024/10 | [GitHub](https://github.com/Cooperx521/PyramidDrop)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers](https://arxiv.org/pdf/2410.14072) <br> Yuxin Wen, Qingqing Cao, Qichen Fu, Sachin Mehta, Mahyar Najibi | 2024/10 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[VidCompress: Memory-Enhanced Temporal Compression for Video Understanding in Large Language Models](https://arxiv.org/pdf/2410.11417) <br> Xiaohan Lan, Yitian Yuan, Zequn Jie, Lin Ma | 2024/10 | - | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models](https://arxiv.org/pdf/2410.07278) <br> Yingen Liu, Fan Wu, Ruihui Li, Zhuo Tang, Kenli Li | 2024/10 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoderr-cyan)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![ICML](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/Gumpest/SparseVLMs.svg?style=social&label=Star)]()<br>[SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](https://arxiv.org/pdf/2410.04417)<br> Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Shanghang Zhang | 2024/10 | [GitHub](https://github.com/Gumpest/SparseVLMs)<br> [Project Page](https://leofan90.github.io/SparseVLMs.github.io/)  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![ICLR](https://img.shields.io/badge/ICLR-2025-blue)]() [![Star](https://img.shields.io/github/stars/rese1f/aurora.svg?style=social&label=Star)]()<br>[AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark](https://arxiv.org/pdf/2410.03051) <br> Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, Christopher D. Manning | 2024/10 | [GitHub](https://github.com/rese1f/aurora)<br> [Project Page](https://wenhaochai.com/aurora-web/) <br> [Model](https://huggingface.co/collections/wchai/auroracap) <br> [Benchmark](https://huggingface.co/datasets/wchai/Video-Detailed-Caption) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![ACL](https://img.shields.io/badge/ACL_Findings-2025-blue)]() [![Star](https://img.shields.io/github/stars/XMUDeepLIT/AVG-LLaVA.svg?style=social&label=Star)]()<br>[AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity](https://arxiv.org/pdf/2410.02745)<br> Zhibin Lan, Liqiang Niu, Fandong Meng, Wenbo Li, Jie Zhou, Jinsong Su | 2024/10 | [GitHub](https://github.com/XMUDeepLIT/AVG-LLaVA)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/VectorSpaceLab/Video-XL.svg?style=social&label=Star)]()<br>[Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding](https://arxiv.org/pdf/2409.14485)<br> Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, Bo Zhao | 2024/09 | [GitHub](https://github.com/VectorSpaceLab/Video-XL)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![AAAI](https://img.shields.io/badge/AAAI-2025-blue)]() [![Star](https://img.shields.io/github/stars/ywh187/FitPrune.svg?style=social&label=Star)]()<br>[Fit and prune: Fast and training-free visual token pruning for multimodal large language models](https://arxiv.org/pdf/2409.10197)<br> Weihao Ye, Qiong Wu, Wenhao Lin, Yiyi Zhou | 2024/09 | [GitHub](https://github.com/ywh187/FitPrune)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![COLING](https://img.shields.io/badge/COLING-2025-blue)]() [![Star](https://img.shields.io/github/stars/FreedomIntelligence/TRIM.svg?style=social&label=Star)]()<br>[Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs](https://arxiv.org/pdf/2409.10994)<br> Dingjie Song, Wenjun Wang, Shunian Chen, Xidong Wang, Michael Guan, Benyou Wang | 2024/09 | [GitHub](https://github.com/FreedomIntelligence/TRIM/)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/NVIDIA/Megatron-LM.svg?style=social&label=Star)]()<br>[NVLM: Open Frontier-Class Multimodal LLMs](https://arxiv.org/pdf/2409.11402) <br> Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping | 2024/09 | [GitHub](https://github.com/NVIDIA/Megatron-LM/tree/NVLM-1.0/examples/multimodal/nvlm)<br> [Project Page](https://research.nvidia.com/labs/adlr/NVLM-1) <br> [Model](https://huggingface.co/nvidia/NVLM-D-72B) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Re--train-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Consideration](https://arxiv.org/pdf/2409.03206) <br> Mingze Gao, Jingyu Liu, Mingda Li, Jiangtao Xie, Qingbin Liu, Bo Zhao, Xi Chen, Hui Xiong | 2024/09 | - | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![AAAI](https://img.shields.io/badge/AAAI-2025-blue)]() [![Star](https://img.shields.io/github/stars/AIDC-AI/TG-LLaVA.svg?style=social&label=Star)]()<br>[TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings](https://arxiv.org/pdf/2409.09564) <br> Dawei Yan, Pengcheng Li, Yang Li, Hao Chen, Qingguo Chen, Weihua Luo, Wei Dong, Qingsen Yan, Haokui Zhang, Chunhua Shen | 2024/09 | [GitHub](https://github.com/AIDC-AI/TG-LLaVA)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![ACL](https://img.shields.io/badge/ACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl.svg?style=social&label=Star)]()<br>[mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding](https://arxiv.org/abs/2409.03420)<br> Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji Zhang, Qin Jin, Fei Huang, Jingren Zhou | 2024/09 | [GitHub](https://github.com/X-PLUG/mPLUG-DocOwl)<br> [Model](https://huggingface.co/mPLUG/DocOwl2)  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![ICLR](https://img.shields.io/badge/ICLR-2025-blue)]() [![Star](https://img.shields.io/github/stars/LunarShen/TempMe.svg?style=social&label=Star)]()<br>[TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval](https://arxiv.org/pdf/2409.01156)<br> Leqi Shen, Tianxiang Hao, Tao He, Sicheng Zhao, Yifeng Zhang, Pengzhang Liu, Yongjun Bao, Guiguang Ding | 2024/09 | [GitHub](https://github.com/LunarShen/TempMe)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![AAAI](https://img.shields.io/badge/AAAI-2025-blue)]() [![Star](https://img.shields.io/github/stars/banjiuyufen/RecoverableCompression.svg?style=social&label=Star)]()<br>[Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information](https://arxiv.org/pdf/2409.01179) <br> Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu | 2024/09 | [GitHub](https://github.com/banjiuyufen/RecoverableCompression)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
|[LVP: Language-guide Visual Projector for Efficient Multimodal LLM](https://openreview.net/pdf?id=PxBzxO02Ef)<br> Anonymous Authors | 2024/09 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star)]()<br>[LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/pdf/2408.03326) <br> Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li | 2024/08 | [GitHub](https://github.com/LLaVA-VL/LLaVA-NeXT) <br> [Project Page](https://llava-vl.github.io/blog/2024-08-05-llava-onevision) <br> [Model](https://huggingface.co/lmms-lab/models) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() | - |
| [![AAAI](https://img.shields.io/badge/AAAI-2025-blue)]() [![Star](https://img.shields.io/github/stars/hasanar1f/HiRED.svg?style=social&label=Star)]()<br>[HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments](https://arxiv.org/pdf/2408.10945) <br> Kazi Hasan Ibn Arif, JinYi Yoon, Dimitrios S. Nikolopoulos, Hans Vandierendonck, Deepu John, Bo Ji | 2024/08 | [GitHub](https://github.com/hasanar1f/HiRED) <br> | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star)]()<br>[mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models](https://arxiv.org/abs/2408.04840)<br> Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou | 2024/08 | [GitHub](https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl3)<br> [Model](https://huggingface.co/mPLUG/mPLUG-Owl3-7B-241101) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() <br> [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[Vote&Mix: Plug-and-Play Token Reduction for Efficient Vision Transformer](https://arxiv.org/pdf/2408.17062)<br> Shuai Peng, Di Fu, Baole Wei, Yong Cao, Liangcai Gao, Zhi Tang | 2024/08 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![ECCV](https://img.shields.io/badge/ECCV-2024-blue)]() [![Star](https://img.shields.io/github/stars/JieShibo/ToCom.svg?style=social&label=Star)]()<br> [Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning](https://arxiv.org/pdf/2408.06798)<br> Shibo Jie, Yehui Tang, Jianyuan Guo, Zhi-Hong Deng, Kai Han, Yunhe Wang | 2024/08 | [GitHub](https://github.com/JieShibo/ToCom)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)](https://arxiv.org/pdf/2408.06840) <br>[Dynamic and Compressive Adaptation of Transformers From Images to Videos](https://arxiv.org/pdf/2408.06840) <br> Guozhen Zhang, Jingyu Liu, Shengming Cao, Xiaotong Zhao, Kevin Zhao, Kai Ma, Limin Wang | 2024/08 | - | [![Area](https://img.shields.io/badge/Video-purple)]() | - |
| [![ECCV](https://img.shields.io/badge/ECCV-2024-blue)]() [![Star](https://img.shields.io/github/stars/username/repo.svg?style=social&label=Star)]()<br>[Instruction Tuning-free Visual Token Complement for Multimodal LLMs](https://arxiv.org/abs/2408.05019)<br> Dongsheng Wang, Jiequan Cui, Miaoge Li, Wang Lin, Bo Chen, Hanwang Zhang | 2024/08 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/apple/ml-slowfast-llava.svg?style=social&label=Star)]()<br>[SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models](https://arxiv.org/pdf/2407.15841) <br> Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, Afshin Dehghan | 2024/07 | [GitHub](https://github.com/apple/ml-slowfast-llava)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/JiuTian-VL/TokenCorrCompressor.svg?style=social&label=Star)]()<br>[Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding](https://arxiv.org/pdf/2407.14439) <br> Renshan Zhang, Yibo Lyu, Rui Shao, Gongwei Chen, Weili Guan, Liqiang Nie | 2024/07 | [GitHub](https://github.com/JiuTian-VL/TokenCorrCompressor)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]() [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() <br>[HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models](https://arxiv.org/pdf/2407.08706) <br> Runhui Huang, Xinpeng Ding, Chunwei Wang, Jianhua Han, Yulong Liu, Hengshuang Zhao, Hang Xu, Lu Hou, Wei Zhang, Xiaodan Liang | 2024/07 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![IJCV](https://img.shields.io/badge/IJCV-2024-blue)]() [![Star](https://img.shields.io/github/stars/CircleRadon/TokenPacker.svg?style=social&label=Star)]()<br>[TokenPacker: Efficient Visual Projector for Multimodal LLM](https://arxiv.org/abs/2407.02392.pdf)<br> Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, Lei Zhang | 2024/07 | [GitHub](https://github.com/CircleRadon/TokenPacker)<br> [Model](https://huggingface.co/collections/sunshine-lwt/tokenpacker) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![ECCV](https://img.shields.io/badge/ECCV-2024-blue)]() <br>[LookupViT: Compressing visual information to a limited number of tokens](https://arxiv.org/pdf/2407.12753)<br> Rajat Koner, Gagan Jain, Prateek Jain, Volker Tresp, Sujoy Paul | 2024/07 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2.svg?style=social&label=Star)]()<br>[VideoLLaMA 2 Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs](https://arxiv.org/pdf/2406.07476) <br> Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, Lidong Bing | 2024/06 | [GitHub](https://github.com/DAMO-NLP-SG/VideoLLaMA2)<br> [Model](https://huggingface.co/collections/DAMO-NLP-SG/videollama2) <br> [Demo](https://huggingface.co/spaces/lixin4ever/VideoLLaMA2) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![Publish](https://img.shields.io/badge/EMNLP_Findings-2024-blue)]() [![Star](https://img.shields.io/github/stars/SUSTechBruce/LOOK-M.svg?style=social&label=Star)]()<br>[LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference](https://arxiv.org/pdf/2406.18139)<br> Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, Li Yuan | 2024/06 | [GitHub](https://github.com/SUSTechBruce/LOOK-M)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![NeurIPS](https://img.shields.io/badge/NeurIPS-2024-blue)]() [![Star](https://img.shields.io/github/stars/Beckschen/LLaVolta.svg?style=social&label=Star)]() <br>[Efficient Large Multi-modal Models via Visual Context Compression](https://arxiv.org/abs/2406.20092)<br> Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, Alan Yuille | 2024/06 | [GitHub](https://github.com/Beckschen/LLaVolta)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/Yxxxb/VoCo-LLaMA.svg?style=social&label=Star)]()<br>[VoCo-LLaMA: Towards Vision Compression with Large Language Models](https://arxiv.org/pdf/2406.12275)<br> Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Yansong Tang | 2024/06 | [GitHub](https://github.com/Yxxxb/VoCo-LLaMA)<br> [Project Page](https://yxxxb.github.io/VoCo-LLaMA-page) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() |
| [![AAAI](https://img.shields.io/badge/AAAI-2025-blue)]() [![Star](https://img.shields.io/github/stars/lzhxmu/VTW.svg?style=social&label=Star)]()<br>[Boosting multimodal large language models with visual tokens withdrawal for rapid inference](https://arxiv.org/pdf/2405.05803) <br> Zhihang Lin, Mingbao Lin, Luxi Lin, Rongrong Ji | 2024/05 | [GitHub](https://github.com/lzhxmu/VTW)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]()<br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/yaolinli/DeCo.svg?style=social&label=Star)]()<br>[DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models](https://arxiv.org/pdf/2405.20985) <br> Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, Lu Hou | 2024/05 | [GitHub](https://github.com/yaolinli/DeCo)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![NeurIPS](https://img.shields.io/badge/NeurIPS-2025-blue)]() [![Star](https://img.shields.io/github/stars/gordonhu608/MQT-LLaVA.svg?style=social&label=Star)]()<br>[Matryoshka Query Transformer for Large Vision-Language Models](https://arxiv.org/pdf/2405.19315)<br> Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, Kai-Wei Chang | 2024/05 | [GitHub](https://github.com/gordonhu608/MQT-LLaVA)<br> [Model](https://huggingface.co/gordonhu/MQT-LLaVA-7b) <br> [Project Page](https://gordonhu608.github.io/mqtllava) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![ICLR](https://img.shields.io/badge/ICLR-2025-blue)]() [![Star](https://img.shields.io/github/stars/mu-cai/matryoshka-mm.svg?style=social&label=Star)]()<br>[Matryoshka Multimodal Models](https://arxiv.org/pdf/2405.17430) <br> Mu Cai, Jianwei Yang, Jianfeng Gao, Yong Jae Lee | 2024/05 | [GitHub](https://github.com/mu-cai/matryoshka-mm)<br> [Project Page](https://matryoshka-mm.github.io/) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![ECCV](https://img.shields.io/badge/ECCV-2024-blue)]() [![Star](https://img.shields.io/github/stars/ziplab/LongVLM.svg?style=social&label=Star)]()<br>[LongVLM: Efficient Long Video Understanding via Large Language Models](https://arxiv.org/pdf/2404.03384) <br> Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, Bohan Zhuang | 2024/04 | [GitHub](https://github.com/ziplab/LongVLM)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/magic-research/PLLaVA.svg?style=social&label=Star)]()<br>[PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning](https://arxiv.org/pdf/2404.16994) <br> Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, Jiashi Feng | 2024/04 | [GitHub](https://github.com/magic-research/PLLaVA)<br> [Project Page](https://pllava.github.io/) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&label=Star)]()<br>[How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/abs/2404.16821.pdf) <br> Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, et al. | 2024/04 | [GitHub](https://github.com/OpenGVLab/InternVL)<br> [Model](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() <br>[CATP: Cross-Attention Token Pruning for Accuracy Preserved Multimodal Model Inference](https://arxiv.org/pdf/2404.08567) <br> Ruqi Liao, Chuqing Zhao, Jin Li, Weiqi Feng | 2024/04 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/pkunlp-icler/FastV.svg?style=social&label=Star)]() <br>[An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models](https://arxiv.org/abs/2403.06764)<br> Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, Baobao Chang | 2024/03 | [GitHub](https://github.com/pkunlp-icler/FastV)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![ICCV](https://img.shields.io/badge/ICCV-2025-blue)]()  [![Star](https://img.shields.io/github/stars/42Shawn/LLaVA-PruMerge.svg?style=social&label=Star)]()<br>[LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388.pdf) <br> Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan | 2024/03 | [GitHub](https://github.com/42Shawn/LLaVA-PruMerge)<br> [Project Page](https://llava-prumerge.github.io/) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]() [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/THU-MIG/PYRA.svg?style=social&label=Star)]()<br>[PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation](https://arxiv.org/abs/2403.09192)<br> Yizhe Xiong, Hui Chen, Tianxiang Hao, Zijia Lin, Jungong Han, Yuesong Zhang, Guoxin Wang, Yongjun Bao, Guiguang Ding | 2024/03 | [GitHub](https://github.com/THU-MIG/PYRA)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]()<br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![CVPR 2024](https://img.shields.io/badge/CVPR-2024-blue)]() [![Star](https://img.shields.io/github/stars/double125/MADTP.svg?style=social&label=Star)]()<br>[MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Vision-Language Tasks](https://arxiv.org/abs/2403.02991)<br> Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, Tao Chen | 2024/03 | [GitHub](https://github.com/double125/MADTP)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Star](https://img.shields.io/github/stars/Meituan-AutoML/MobileVLM.svg?style=social&label=Star)]()<br>[MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766.pdf) <br> Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, Chunhua Shen | 2024/02 | [GitHub](https://github.com/Meituan-AutoML/MobileVLM)<br> [Model](https://huggingface.co/mtgv) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() [![Train_Infer](https://img.shields.io/badge/Infer-yellowgreen)]() |
| [![CVPR 2024](https://img.shields.io/badge/CVPR-2024-blue)]() [![Star](https://img.shields.io/github/stars/khanrc/honeybee.svg?style=social&label=Star)]()<br>[Honeybee: Locality-enhanced Projector for Multimodal LLM](https://arxiv.org/abs/2312.06742)<br> Junbum Cha, Wooyoung Kang, Jonghwan Mun, Byungseok Roh | 2023/12 | [GitHub](https://github.com/khanrc/honeybee)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![CVPR 2024](https://img.shields.io/badge/CVPR-2024-blue)]() [![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi.svg?style=social&label=Star)]()<br>[Chat-univi: Unified visual representation empowers large language models with image and video understanding](https://arxiv.org/pdf/2311.08046) <br> Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, Li Yuan | 2023/11 | [GitHub](https://github.com/PKU-YuanGroup/Chat-UniVi)<br> [Model](https://huggingface.co/Chat-UniVi) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![ECCV](https://img.shields.io/badge/ECCV-2024-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/LLaMA-VID.svg?style=social&label=Star)]()<br>[LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043)<br> Yanwei Li, Chengyao Wang, Jiaya Jia | 2023/11 | [GitHub](https://github.com/dvlab-research/LLaMA-VID)<br> [Project Page](https://llama-vid.github.io/) <br> [Model](https://huggingface.co/YanweiLi) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Fix-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![EMNLP+2023+findings](https://img.shields.io/badge/EMNLP_Findings-2023-blue)]() [![Star](https://img.shields.io/github/stars/RenShuhuai-Andy/TESTA.svg?style=social&label=Star)]() <br>[TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding](https://arxiv.org/pdf/2310.19060)<br> Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, Lu Hou | 2023/10 | [GitHub](https://github.com/RenShuhuai-Andy/TESTA)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() <br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() <br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2023-red)]() [![Star](https://img.shields.io/github/stars/xjwu1024/PPT.svg?style=social&label=Star)]()<br>[PPT: Token Pruning and Pooling for Efficient Vision Transformers](https://arxiv.org/pdf/2310.01812) <br> Xinjian Wu, Fanhu Zeng, Xiudong Wang, Xinghao Chen | 2023/10 | [GitHub](https://github.com/xjwu1024/PPT)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Ratio](https://img.shields.io/badge/Dynamic-pink)]() <br> [![Train_Infer](https://img.shields.io/badge/Train-yellowgreen)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2023-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen-VL.svg?style=social&label=Star)]()<br>[Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/pdf/2308.12966v2)<br> Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou | 2023/08 | [GitHub](https://github.com/QwenLM/Qwen-VL)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Cross_Attention-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![CVPR](https://img.shields.io/badge/CVPR-2024-blue)]() [![Star](https://img.shields.io/github/stars/rese1f/MovieChat.svg?style=social&label=Star)]()<br>[MovieChat: From Dense Token to Sparse Memory for Long Video Understanding](https://arxiv.org/pdf/2307.16449v4)<br> Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, Yan Lu, Jenq-Neng Hwang, Gaoang Wang | 2023/07 | [GitHub](https://github.com/rese1f/MovieChat)<br> [Project Page](https://wenhaochai.com/MovieChat) | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![ACL 2024](https://img.shields.io/badge/ACL-2024-blue)]() [![Star](https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&label=Star)]()<br>[Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models](https://arxiv.org/pdf/2306.05424)<br> Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan | 2023/06 | [GitHub](https://github.com/mbzuai-oryx/Video-ChatGPT)<br>  | [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![ACL 2023](https://img.shields.io/badge/ACL-2023-blue)]() [![Star](https://img.shields.io/github/stars/csarron/PuMer.svg?style=social&label=Star)]()<br>[PuMer: Pruning and Merging Tokens for Efficient Vision Language Models](https://arxiv.org/pdf/2305.17530)<br> Qingqing Cao, Bhargavi Paranjape, Hannaneh Hajishirzi | 2023/05 | [GitHub](https://github.com/csarron/PuMer)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br>[![Method](https://img.shields.io/badge/Merge-lightgrey)]() [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br>[![Approach](https://img.shields.io/badge/Re--train-yellow)]()<br>[![Speed](https://img.shields.io/badge/Speed--Train-orange)]() [![Speed](https://img.shields.io/badge/Speed--Infer-orange)]() |
| [![NeurIPS 2023](https://img.shields.io/badge/NeurIPS-2023-blue)]() [![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)]()<br>[InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/pdf/2305.06500)<br> Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi | 2023/05 | [GitHub](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() |
| [![ICML 2024](https://img.shields.io/badge/ICML-2024-blue)]() [![Star](https://img.shields.io/github/stars/sdc17/CrossGET.svg?style=social&label=Star)]()<br>[CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers](https://arxiv.org/pdf/2305.17455v4)<br> Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, Jiaqi Wang | 2023/05 | [GitHub](https://github.com/sdc17/CrossGET)<br> [Model](https://drive.google.com/drive/folders/1E1Qegfy1yeBUwX6rXW6q6kakb3VPtXxU?usp=sharing) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoderr-cyan)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![COLING 2024](https://img.shields.io/badge/COLING-2024-blue)]() <br>[SmartTrim: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models](https://arxiv.org/abs/2305.15033)<br> Zekun Wang, Jingchang Chen, Wangchunshu Zhou, Haichao Zhu, Jiafeng Liang, Liping Shan, Ming Liu, Dongliang Xu, Qing Yang, Bing Qin | 2023/05 | - | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Vision_Encoder-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Pruning-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2023-red)]() [![Star](https://img.shields.io/github/stars/OpenGVLab/DiffRate.svg?style=social&label=Star)]()<br>[DiffRate: Differentiable Compression Rate for Efficient Vision Transformers](https://arxiv.org/abs/2305.17997)<br> Mengzhao Chen, Wenqi Shao, Peng Xu, Mingbao Lin, Kaipeng Zhang, Fei Chao, Rongrong Ji, Yu Qiao, Ping Luo | 2023/05 | [GitHub](https://github.com/OpenGVLab/DiffRate)<br>  | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2023-red)](https://arxiv.org/pdf/2304.10592) [![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star)]()<br>[MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/pdf/2304.10592)<br> Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny | 2023/04 | [GitHub](https://github.com/Vision-CAIR/MiniGPT-4)<br> [Project Page](https://minigpt-4.github.io) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2023-red)]() [![Star](https://img.shields.io/github/stars/megvii-research/TPS-CVPR2023.svg?style=social&label=Star)]()<br>[Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers](https://arxiv.org/pdf/2304.10716) <br> Siyuan Wei, Tianzhu Ye, Shen Zhang, Yao Tang, Jiajun Liang | 2023/04 | [GitHub](https://github.com/megvii-research/TPS-CVPR2023)<br>  | - | - |
| [![ICML 2023](https://img.shields.io/badge/ICML-2023-blue)]() [![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)]()<br>[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)<br> Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi | 2023/01 | [GitHub](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)<br> | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/Projector-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() |
| [![CVPR 2023](https://img.shields.io/badge/CVPR-2023-blue)]() [![Star](https://img.shields.io/github/stars/google-research/scenic.svg?style=social&label=Star)]()<br>[Token Turing Machines](https://arxiv.org/pdf/2211.09119)<br> Michael S. Ryoo, Keerthana Gopalakrishnan, Kumara Kahatapitiya, Ted Xiao, Kanishka Rao, Austin Stone, Yao Lu, Julian Ibarz, Anurag Arnab | 2022/11 | [GitHub](https://github.com/google-research/scenic/tree/main/scenic/projects/token_turing)<br>  | [![Area](https://img.shields.io/badge/Image-purple)]() [![Area](https://img.shields.io/badge/Video-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![ICLR 2023](https://img.shields.io/badge/ICLR-2023-blue)]() [![Star](https://img.shields.io/github/stars/facebookresearch/ToMe.svg?style=social&label=Star)]()<br>[Token Merging: Your ViT But Faster](https://arxiv.org/pdf/2210.09461)<br> Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, Judy Hoffman | 2022/10 | [GitHub](https://github.com/facebookresearch/ToMe)<br>  | [![Image](https://img.shields.io/badge/Image-purple)]() [![Early ViT Compression](https://img.shields.io/badge/ViT-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-No-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]() [![Approach](https://img.shields.io/badge/Plug_In-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Infer-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2022-red)](https://arxiv.org/pdf/2209.13802)<br>[Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention](https://arxiv.org/pdf/2209.13802)<br> Xiangcheng Liu, Tianyi Wu, Guodong Guo | 2022/09 | - | - | - |
| [![NeurIPS 2022](https://img.shields.io/badge/NeurIPS-2022-blue)]() [![Star](https://img.shields.io/github/stars/lucidrains/flamingo-pytorch.svg?style=social&label=Star)]()<br>[Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/pdf/2204.14198)<br> Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. | 2022/04 | [GitHub](https://github.com/lucidrains/flamingo-pytorch) | [![Area](https://img.shields.io/badge/Image-purple)]() [![Stage](https://img.shields.io/badge/LLM-cyan)]() | [![Text Query](https://img.shields.io/badge/TQ-Yes-brightgreen)]()<br> [![Method](https://img.shields.io/badge/Merge-lightgrey)]()<br> [![Approach](https://img.shields.io/badge/Retrain-yellow)]()<br> [![Speed](https://img.shields.io/badge/Speed_Train-orange)]() |
| [![arXiv](https://img.shields.io/badge/arXiv-2022-red)]() [![Star](https://img.shields.io/github/stars/youweiliang/evit.svg?style=social&label=Star)](https://github.com/your-repo-link)<br>[EViT: Expediting Vision Transformers via Token Reorganizations](https://arxiv.org/pdf/2202.07800) <br> Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, Pengtao Xie | 2022/02 | [GitHub](https://github.com/youweiliang/evit)<br>  | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2022-red)]() [![Star](https://img.shields.io/github/stars/Arnav0400/ViT-Slim.svg?style=social&label=Star)]()<br>[Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space](https://arxiv.org/pdf/2201.00814) <br> Arnav Chavan, Zhiqiang Shen, Zhuang Liu, Zechun Liu, Kwang-Ting Cheng, Eric Xing | 2022/01 | [GitHub](https://github.com/Arnav0400/ViT-Slim)<br>  | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2021-red)]() [![Star](https://img.shields.io/github/stars/NVlabs/A-ViT.svg?style=social&label=Star)]()<br>[A-ViT: Adaptive Tokens for Efficient Vision Transformer](https://arxiv.org/pdf/2112.07658) <br> Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, Pavlo Molchanov | 2021/12 | [GitHub](https://github.com/NVlabs/A-ViT)<br> [Project Page](https://a-vit.github.io/) | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2021-red)]() [![Star](https://img.shields.io/github/stars/adaptivetokensampling/ATS.svg?style=social&label=Star)]()<br>[ATS: Adaptive Token Sampling For Efficient Vision Transformers](https://arxiv.org/abs/2111.15667) <br> Mohsen Fayyaz, Soroush Abbasi Koohpayegani, Farnoush Rezaei Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, Juergen Gall | 2021/11 | [GitHub](https://github.com/adaptivetokensampling/ATS)<br> [Project Page](https://adaptivetokensampling.github.io/) | - | - |
| [![AAAI 2023](https://img.shields.io/badge/AAAI-2022-blue)]() [![Star](https://img.shields.io/github/stars/YifanXu74/Evo-ViT.svg?style=social&label=Star)]()<br>[Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer](https://arxiv.org/abs/2108.01390)<br> Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng Xu, Xing Sun | 2021/08 | [GitHub](https://github.com/YifanXu74/Evo-ViT)<br>  | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2021-red)]() <br>[Patch Slimming for Efficient Vision Transformers](https://arxiv.org/abs/2106.02852) <br> Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, Dacheng Tao | 2021/06 | - | - | - |
| [![arXiv](https://img.shields.io/badge/arXiv-2021-red)]() [![Star](https://img.shields.io/github/stars/raoyongming/DynamicViT.svg?style=social&label=Star)]()<br>[DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification](https://arxiv.org/abs/2106.02034)<br> Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, Cho-Jui Hsieh | 2021/06 | [GitHub](https://github.com/raoyongming/DynamicViT)<br> [Project Page](https://dynamicvit.ivg-research.xyz/) | - | - |

## üìà BenchmarkÔºàComing SoonÔºâ
We compiled the image and video understanding benchmarks commonly used in token pruning studies, and built a comprehensive evaluation framework based on them. Through our framework, users can **evaluate 26 relevant benchmarks (15 image-based and 11 video-based) in a single pass**, which helps provide an overview of a method's systemic capabilities.

The dataset and evaluation scripts are ready and will **be released here shortly**.

## üìå Citation

If you find our paper or this resource helpful, please consider cite:

```bibtex
@article{xxx}
```

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---